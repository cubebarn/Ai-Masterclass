{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFwIONSJ0LgUo5kdLv5zSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cubebarn/Ai-Masterclass/blob/main/Attention_Block.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Z68jlyUxyF2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step of implementing self-attention is to compute the intermediate values ω,\n",
        "referred to as attention scores"
      ],
      "metadata": {
        "id": "OpllNXCJyEzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] #A\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(x_i, query)\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5HDioIRyKMR",
        "outputId": "f6d25bcc-27af-4ba4-cc5d-ce1bcb545173"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dot product is essentially just a concise way of multiplying two vectors elementwise and then summing the products, which we can demonstrate as follows:\n"
      ],
      "metadata": {
        "id": "GtaXiFf80NVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| a    | b    | a×b    |\n",
        "| ---- | ---- | ------ |\n",
        "| 0.43 | 0.55 | 0.2365 |\n",
        "| 0.15 | 0.87 | 0.1305 |\n",
        "| 0.89 | 0.66 | 0.5874 |\n",
        "\n",
        "\n",
        "dot=0.2365+0.1305+0.5874=0.9544"
      ],
      "metadata": {
        "id": "_gdzdD170qeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = 0.\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "  res += inputs[0][idx] * query[idx]\n",
        "print(res)\n",
        "print(torch.dot(inputs[0], query))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y0qIpta0DEw",
        "outputId": "e55fd2f1-944f-4882-a1d5-6a09003620fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9544)\n",
            "tensor(0.9544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " After computing the attention scores ω21\n",
        "to ω2T with respect to the input query x\n",
        "(2)\n",
        ", the next step is\n",
        "to obtain the attention weights α21\n",
        "to α2T by normalizing the attention scores.\n"
      ],
      "metadata": {
        "id": "VQVhnW6n1CIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "print(\"Attention weights:\", attn_weights_2_tmp)\n",
        "print(\"Sum:\", attn_weights_2_tmp.sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtisI-U-1DmP",
        "outputId": "3c62eeff-5885-43bd-90c4-d36f90be2bf1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " it's more common and advisable to use the softmax function for normalization.\n",
        "This approach is better at managing extreme values and offers more favorable gradient\n",
        "properties during training."
      ],
      "metadata": {
        "id": "QB1TGusI1xXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "print(\"Sum:\", attn_weights_2_naive.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_mBy2g-1yUb",
        "outputId": "e83f34b6-487b-4c21-b9c4-e620363c4462"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the output shows, the softmax function also meets the objective and normalizes the\n",
        "attention weights such that they sum to 1:\n",
        "\n",
        "\n",
        "In addition, the softmax function ensures that the attention weights are always positive.\n",
        "This makes the output interpretable as probabilities or relative importance, where higher\n",
        "weights indicate greater importance.\n"
      ],
      "metadata": {
        "id": "bZ2C4U5X2HJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, in practice, it's advisable to use the PyTorch implementation of softmax,\n",
        "which has been extensively optimized for performance:"
      ],
      "metadata": {
        "id": "Kgedr7Xo2X6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJQM5Ies2IU9",
        "outputId": "5fc6226b-4f9f-449f-ae5d-169b71fae7d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating the context vector z\n",
        "(2) by multiplying the embedded\n",
        "input tokens, x\n",
        "(i)\n",
        ", with the corresponding attention weights and then summing the resulting\n",
        "vectors.\n"
      ],
      "metadata": {
        "id": "vCm1FU0q2j6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step, after calculating and normalizing the attention scores to obtain the attention\n",
        "weights for query x\n",
        "(2)\n",
        ", is to compute the context vector"
      ],
      "metadata": {
        "id": "LM_n-fAB37PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] # 2nd input token is the query\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "  context_vec_2 += attn_weights_2[i]*x_i\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPh7GPuz3-j4",
        "outputId": "f1543248-2d57-464c-df02-cfb88faa4721"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting attention scores are as follows:"
      ],
      "metadata": {
        "id": "59QSd-1t5HVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMcsd0JF5Bn9",
        "outputId": "5e43eebe-7f90-4fda-852f-e2c6f439f815"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpITp8le5aqw",
        "outputId": "f20f7b3b-7273-4d44-cc99-ac2a24105fc6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we now normalize each row so that the values in\n",
        "each row sum to 1:"
      ],
      "metadata": {
        "id": "WoVRxSPj5hN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yolYW-8_5ni3",
        "outputId": "d8ea28ef-2f38-4598-a879-a4c1c945788f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move on to step 3, the final step shown in Figure 3.12, let's briefly verify that\n",
        "the rows indeed all sum to 1:"
      ],
      "metadata": {
        "id": "N30zHG926dow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqPgNBO6hVP",
        "outputId": "7971917d-9cf7-4a1b-8803-906d5040829a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the third and last step, we now use these attention weights to compute all context\n",
        "vectors via matrix multiplication:"
      ],
      "metadata": {
        "id": "oWPqzCGY6nyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqRq__Kh6o9S",
        "outputId": "11fd85fb-827c-4777-d5ea-09d5d49cb09f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Implementing self-attention with trainable weights"
      ],
      "metadata": {
        "id": "R2vVybL561fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement the self-attention mechanism step by step by introducing the three\n",
        "trainable weight matrices Wq\n",
        ", Wk\n",
        ", and Wv\n",
        ". These three matrices are used to project the"
      ],
      "metadata": {
        "id": "_BeCOUof6-FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2"
      ],
      "metadata": {
        "id": "QpOsBZlG7a7w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we initialize the three weight matrices Wq\n",
        ", Wk\n",
        ", and Wv"
      ],
      "metadata": {
        "id": "dzjcn8Sw7mkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "_Nm1V7N27xuh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we are setting requires_grad=False to reduce clutter in the outputs for\n",
        "illustration purposes, but if we were to use the weight matrices for model training, we\n",
        "would set requires_grad=True to update these matrices during model training.\n"
      ],
      "metadata": {
        "id": "ooYO0nep8YMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we compute the query, key, and value vectors"
      ],
      "metadata": {
        "id": "ns0gW3kJ8ZxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xiIkLon8fma",
        "outputId": "e17b9cf3-b5be-41b8-b5a2-fcf95a402da1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see based on the output for the query, this results in a 2-dimensional vector\n",
        "since we set the number of columns of the corresponding weight matrix, via d_out, to 2"
      ],
      "metadata": {
        "id": "K4pyh8jl8nYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though our temporary goal is to only compute the one context vector, z\n",
        "(2)\n",
        ", we still\n",
        "require the key and value vectors for all input elements as they are involved in computing\n",
        "the attention weights with respect to the query q\n",
        "\n",
        "We can obtain all keys and values via matrix multiplication:\n"
      ],
      "metadata": {
        "id": "nt2AG3fF86kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXd5Fh_m9Jho",
        "outputId": "72983c45-4ef4-491e-d3b6-2155f8fb3615"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can tell from the outputs, we successfully projected the 6 input tokens from a 3D\n",
        "onto a 2D embedding space:"
      ],
      "metadata": {
        "id": "yCdaOEwJ9T9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's compute the attention score ω22\n",
        ":"
      ],
      "metadata": {
        "id": "Y5FHJ2Dc9bhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1] #A\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeHqXkqu9er-",
        "outputId": "e46c8f7b-4e12-49be-c129-9322d1e4bfc1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can generalize this computation to all attention scores via matrix multiplication:"
      ],
      "metadata": {
        "id": "KlZpqbDJ9naP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu6nlqLQ9rdh",
        "outputId": "5f491d41-82f1-4b6b-96a1-0acdc1710b07"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third step is now going from the attention scores to the attention weights"
      ],
      "metadata": {
        "id": "gYbj8Y469yEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After computing the attention scores ω, the next step is to normalize these scores using the\n",
        "softmax function to obtain the attention weights α."
      ],
      "metadata": {
        "id": "2-ElV1xo961Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV61Y5UW9_c8",
        "outputId": "806c15b8-e8e2-4fb1-dbb1-0eb147b63f29"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " In the final step of the self-attention computation, we compute the context vector by combining all\n",
        "value vectors via the attention weights."
      ],
      "metadata": {
        "id": "4ZD5et4d-ene"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65E2q_Qb-hnA",
        "outputId": "30f3e94b-6a21-47b2-ef18-db06c995c23f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "  def forward(self, x):\n",
        "    keys = x @ self.W_key\n",
        "    queries = x @ self.W_query\n",
        "    values = x @ self.W_value\n",
        "    attn_scores = queries @ keys.T # omega\n",
        "    attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "sF19biRU_Tj_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFXRUQ_RAOVg",
        "outputId": "a2b62504-0764-450c-92d9-7baf6c11f380"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, a significant advantage of using nn.Linear instead of manually\n",
        "implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
        "initialization scheme, contributing to more stable and effective model training.\n"
      ],
      "metadata": {
        "id": "BjuJgngBApG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "ocqxfwfzAUPu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2p8JWQZA5g6",
        "outputId": "b8b68559-755f-4b1d-b6ab-7c7a41a99a22"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v2.W_query(inputs) #A\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zow5PcSWBpuG",
        "outputId": "d3980032-2d87-40c8-8026-bb3f5a432395"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can implement step 2 using PyTorch's tril function to create a mask\n",
        "where the values above the diagonal are zero:"
      ],
      "metadata": {
        "id": "jpFX7Qx5Bzmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xka3XMgB3Cg",
        "outputId": "136650f0-5d9d-463c-def4-08f9af40906f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can multiply this mask with the attention weights to zero out the values above the\n",
        "diagonal:\n"
      ],
      "metadata": {
        "id": "aU1N3rKRCFg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGLNG9AzCG4u",
        "outputId": "810e0d27-d977-4801-e393-487c8f543ebf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third step is to renormalize the attention weights to sum up to 1 again in\n",
        "each row. We can achieve this by dividing each element in each row by the sum in each\n",
        "row:"
      ],
      "metadata": {
        "id": "odfaX_qhCOjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmvO-PVgCUnr",
        "outputId": "b2e7147d-76bf-4f5b-cfc3-4980d46f3dd1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is an attention weight matrix where the attention weights above the diagonal are\n",
        "zeroed out and where the rows sum to 1:\n"
      ],
      "metadata": {
        "id": "BymMhU2NCblX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax function converts its inputs into a probability distribution. When negative\n",
        "infinity values (-∞) are present in a row, the softmax function treats them as zero\n",
        "probability. (Mathematically, this is because e\n",
        "-∞ approaches 0.)\n",
        "We can implement this more efficient masking \"trick\" by creating a mask with 1's above\n",
        "the diagonal and then replacing these 1's with negative infinity (-inf) values:"
      ],
      "metadata": {
        "id": "DjsuSbO4CwZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJkQXTPfC0F3",
        "outputId": "7a47ba6b-70ae-4d49-8c34-ef0fe91371a6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, all we need to do is apply the softmax function to these masked results, and we are\n",
        "done:"
      ],
      "metadata": {
        "id": "0NgYZ8U8C54W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7nwqri3C9h-",
        "outputId": "6be45097-9998-4c47-ad2d-16ea7ccb341f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) #A\n",
        "example = torch.ones(6, 6) #B\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33RWAo6oDeU_",
        "outputId": "374319db-230d-43fc-b369-288f2f9c4602"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A We choose a dropout rate of 50%\n",
        "#B Here, we create a matrix of 1's"
      ],
      "metadata": {
        "id": "1WBMrtpmDj8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WN1VY3DD0YV",
        "outputId": "9817dcd8-fc80-4617-a1c4-8db79f34c252"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73mk5I6BEEh9",
        "outputId": "283507ec-7ab6-440e-f79e-7b333ff9c898"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout) #A\n",
        "    self.register_buffer(\n",
        "    'mask',\n",
        "    torch.triu(torch.ones(context_length, context_length),\n",
        "    diagonal=1)\n",
        "    ) #B\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape #C\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "    attn_scores = queries @ keys.transpose(1, 2) #C\n",
        "    attn_scores.masked_fill_( #D\n",
        "    self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "JJ96ipVBEzDc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A Compared to the previous SelfAttention_v1 class, we added a dropout layer\n",
        "#B The register_buffer call is also a new addition (more information is provided in the following text)\n",
        "#C We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0)\n",
        "#D In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory\n",
        "copies\n",
        "\n",
        "\n",
        "Some tensors are important for the model but shouldn’t be trained—like:\n",
        "\n",
        "masks (e.g., causal attention masks)\n",
        "\n",
        "running stats (batch norm running mean/var)\n",
        "\n",
        "positional encodings\n",
        "\n",
        "constants used repeatedly (like π, scale factors)\n",
        "\n",
        "register_buffer keeps them inside the module properly."
      ],
      "metadata": {
        "id": "ZvRoCTWdFONV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oejMCNDGFkTG",
        "outputId": "980875f3-bd06-4e3a-f45a-3901ba622bce"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head Attention\n"
      ],
      "metadata": {
        "id": "eUOkI2-8FvCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "CqhVx2THFx63"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n",
        "[0.8993, 0.0390, 0.9268, 0.7388],\n",
        "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "[0.4066, 0.2318, 0.4545, 0.9737],\n",
        "[0.4606, 0.5159, 0.4220, 0.5786]]]])\n"
      ],
      "metadata": {
        "id": "i8qCv1PhGu0L"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a @ a.transpose(2, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOYwEweTIuYk",
        "outputId": "72c5916f-c664-47ea-eae5-607780a7f82f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_head = a[0, 0, :, :]\n",
        "first_res = first_head @ first_head.T\n",
        "print(\"First head:\\n\", first_res)\n",
        "second_head = a[0, 1, :, :]\n",
        "second_res = second_head @ second_head.T\n",
        "print(\"\\nSecond head:\\n\", second_res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtedSLYxIymz",
        "outputId": "62227000-a16b-4cc5-971c-a70de218591c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First head:\n",
            " tensor([[1.3208, 1.1631, 1.2879],\n",
            "        [1.1631, 2.2150, 1.8424],\n",
            "        [1.2879, 1.8424, 2.0402]])\n",
            "\n",
            "Second head:\n",
            " tensor([[0.4391, 0.7003, 0.5903],\n",
            "        [0.7003, 1.3737, 1.0620],\n",
            "        [0.5903, 1.0620, 0.9912]])\n"
          ]
        }
      ]
    }
  ]
}